{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb1a74-e563-4bb8-9d38-5baa52882aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import substring\n",
    "\n",
    "# Initiate Spark session\n",
    "spark = SparkSession.builder.appName(\"Pizza\").getOrCreate()\n",
    "\n",
    "# Ingest CSV files with optimized partitioning and caching\n",
    "pizza_path = 'dbfs:/FileStore/pizzas.csv'\n",
    "pizza_type_path = 'dbfs:/FileStore/pizza_types_latest.csv'\n",
    "orders_path = 'dbfs:/FileStore/orders.csv'\n",
    "orders_details_path = 'dbfs:/FileStore/order_details.csv'\n",
    "\n",
    "# Load CSVs with repartitioning based on expected data size and Spark cluster\n",
    "df_pizza = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", True) \\\n",
    "  .option(\"inferSchema\", True) \\\n",
    "  .option(\"delimiter\", \",\") \\\n",
    "  .load(pizza_path) \\\n",
    "  .repartition(4)  # Adjust partition based on data size\n",
    "\n",
    "df_pizza_type = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", True) \\\n",
    "  .option(\"inferSchema\", True) \\\n",
    "  .option(\"delimiter\", \",\") \\\n",
    "  .load(pizza_type_path) \\\n",
    "  .repartition(4)\n",
    "\n",
    "df_orders = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", True) \\\n",
    "  .option(\"inferSchema\", True) \\\n",
    "  .option(\"delimiter\", \",\") \\\n",
    "  .load(orders_path) \\\n",
    "  .withColumn(\"time\", substring(\"time\", 12, 8)) \\\n",
    "  .repartition(4)\n",
    "\n",
    "df_order_details = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", True) \\\n",
    "  .option(\"inferSchema\", True) \\\n",
    "  .option(\"delimiter\", \",\") \\\n",
    "  .load(orders_details_path) \\\n",
    "  .repartition(4)\n",
    "\n",
    "# Save tables with specific partitioning if needed for large datasets\n",
    "df_pizza.write.mode(\"overwrite\").saveAsTable(\"PIZZA\")\n",
    "df_pizza_type.write.mode(\"overwrite\").saveAsTable(\"PIZZA_TYPES\")\n",
    "df_orders.write.mode(\"overwrite\").saveAsTable(\"PIZZA_ORDERS\")\n",
    "df_order_details.write.mode(\"overwrite\").saveAsTable(\"PIZZA_ORDER_DTL\")\n",
    "\n",
    "# Verify data counts\n",
    "for table_name in [\"PIZZA\", \"PIZZA_TYPES\", \"PIZZA_ORDERS\", \"PIZZA_ORDER_DTL\"]:\n",
    "    spark.sql(f\"SELECT COUNT(*) AS {table_name} FROM {table_name}\").show()\n",
    "\n",
    "# Cache tables if used multiple times to reduce re-computation cost\n",
    "spark.catalog.cacheTable(\"PIZZA\")\n",
    "spark.catalog.cacheTable(\"PIZZA_ORDER_DTL\")\n",
    "spark.catalog.cacheTable(\"PIZZA_TYPES\")\n",
    "\n",
    "# Identify the most common pizza size ordered\n",
    "df_most_common_size = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT size, COUNT(*) AS sold_count_by_size\n",
    "    FROM (\n",
    "        SELECT dtl.pizza_id, p.size\n",
    "        FROM PIZZA_ORDER_DTL dtl\n",
    "        LEFT JOIN PIZZA p ON dtl.pizza_id = p.pizza_id\n",
    "    ) AS joined_data\n",
    "    GROUP BY size\n",
    "    ORDER BY sold_count_by_size DESC\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "df_most_common_size = df_most_common_size.repartition(1)  # Repartition based on small result size\n",
    "df_most_common_size.show()\n",
    "\n",
    "# List the top 5 most ordered pizza types along with their quantities\n",
    "df_top_5 = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT pt.name, ordered_data.quantity\n",
    "    FROM (\n",
    "        SELECT dtl.pizza_id, SUM(dtl.quantity) AS quantity\n",
    "        FROM PIZZA_ORDER_DTL dtl\n",
    "        GROUP BY dtl.pizza_id\n",
    "        ORDER BY quantity DESC\n",
    "        LIMIT 5\n",
    "    ) AS ordered_data\n",
    "    LEFT JOIN PIZZA p ON ordered_data.pizza_id = p.pizza_id\n",
    "    LEFT JOIN PIZZA_TYPES pt ON p.pizza_type_id = pt.pizza_type_id\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "df_top_5 = df_top_5.repartition(1)  # Repartition based on small result size\n",
    "df_top_5.show()\n",
    "\n",
    "# Un-cache tables when done\n",
    "spark.catalog.uncacheTable(\"PIZZA\")\n",
    "spark.catalog.uncacheTable(\"PIZZA_ORDER_DTL\")\n",
    "spark.catalog.uncacheTable(\"PIZZA_TYPES\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
